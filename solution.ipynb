{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GoTo ML\n",
    "This is a solution for qualifying problem for GoTo ML school\n",
    "\n",
    "6 different approaches are implemented below:\n",
    "+ dummy: random prediction \n",
    "+ mk0: baseline solution from the first\n",
    "+ mk1: statistical solution based on finding similarity between user and film in films.json feature space\n",
    "+ mk2: SVD matrix decomposition\n",
    "+ mk3: Random forest tree classification on top-k features\n",
    "+ mk4: Neural net regression on top-k features\n",
    "\n",
    "## Parts:\n",
    "1. Data precalc\n",
    "2. Defining first-teir solutions\n",
    "2. More data precalc\n",
    "3. Defining second-teir solutions\n",
    "4. Quality metrics\n",
    "\n",
    "## Functions\n",
    "There are two common types of function:\n",
    "+ user_film_NAME function - by user and item returns their similariy\n",
    "+ recommend_NAME function - by user returns top-k recommendations for user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import json\n",
    "from math import sqrt\n",
    "from metrics import apk\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data precalc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "user_to_items = defaultdict(set)\n",
    "item_to_users = defaultdict(set)\n",
    "edges = []\n",
    "#Test\n",
    "test_user_to_items = defaultdict(set)\n",
    "test_item_to_users = defaultdict(set)\n",
    "test_edges = []\n",
    "\n",
    "with open(\"data/train_likes.csv\") as datafile:\n",
    "    for like in csv.DictReader(datafile):\n",
    "        # Every like goes to train or test\n",
    "        if random.random() < 0.5:\n",
    "            user_to_items[like['user_id']].add(like['item_id'])\n",
    "            item_to_users[like['item_id']].add(like['user_id'])\n",
    "            edges.append((like['user_id'], like['item_id']))\n",
    "        else:\n",
    "            test_user_to_items[like['user_id']].add(like['item_id'])\n",
    "            test_item_to_users[like['item_id']].add(like['user_id'])\n",
    "            test_edges.append((like['user_id'], like['item_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_items = set(item_to_users.keys()) | set(test_item_to_users.keys())\n",
    "all_users = set(user_to_items.keys()) | set(test_user_to_items.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_list = list(user_to_items.keys())\n",
    "test_users_list = list(test_user_to_items.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_to_i = {user: i for i, user in enumerate(all_users)}\n",
    "item_to_i = {item: i for i, item in enumerate(all_items)}\n",
    "all_users_list = list(all_users)\n",
    "all_items_list = list(all_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix = sp.sparse.lil_matrix((len(all_users), len(all_items)))\n",
    "test_matrix = sp.sparse.lil_matrix((len(all_users), len(all_items)))\n",
    "for user, items in user_to_items.items():\n",
    "    for item in items:\n",
    "        matrix[user_to_i[user], item_to_i[item]] = True\n",
    "for user, items in test_user_to_items.items():\n",
    "    for item in items:\n",
    "        test_matrix[user_to_i[user], item_to_i[item]] = True\n",
    "matrix = matrix.tocsr()\n",
    "test_matrix = test_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u, s, vt = sp.sparse.linalg.svds(matrix.astype(np.float32), k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u.shape, s.shape, vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "films = json.load(open('data/items.json'))\n",
    "films = {a['id']:a for a in films}\n",
    "for a in films.values():\n",
    "    del a['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in films.values():\n",
    "    if 'genre' in f:\n",
    "        f[f['genre']] = 1\n",
    "        del f['genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finding out what features are more popular\n",
    "features = Counter()\n",
    "for f in films.values():\n",
    "    features.update(set(f.keys()))\n",
    "features.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_features_size = 60 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Map feature to it's number\n",
    "small_features = set([_[0] for _ in features.most_common(small_features_size)])\n",
    "feature_to_i = {feature: i for i, feature in enumerate(small_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#User filtration\n",
    "min_items_per_user = 2\n",
    "from copy import copy\n",
    "for user in copy(test_user_to_items).keys():\n",
    "    \n",
    "    n_items_per_user = len(user_to_items[user]) + len(test_user_to_items[user])\n",
    "    \n",
    "    if n_items_per_user <= min_items_per_user:\n",
    "        del user_to_items[user]\n",
    "        del test_user_to_items[user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_film_mk2(user, item, debug=False):\n",
    "    ui = user_to_i[user]\n",
    "    ii = item_to_i[item]\n",
    "    if debug:\n",
    "        print(ui, ii)\n",
    "    a = np.dot(u[ui,:] * s, vt[:,ii])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generic_recommend(user_flim_function):\n",
    "    'Funstion return recomend_NAME function by user_flim_NAME function'\n",
    "    def recommend(user, n_best = 10, debug=False):\n",
    "        user_items = user_to_items[user]\n",
    "\n",
    "        item_similarities = {}\n",
    "        for item in all_items:\n",
    "            if item in user_items: continue\n",
    "            item_users = item_to_users[item]\n",
    "            if len(item_users) == 0: continue\n",
    "\n",
    "            item_similarities[item] = user_flim_function(user, item)\n",
    "\n",
    "        items_sorted = sorted(all_items, key = lambda x: item_similarities.get(x, 0),reverse = True)\n",
    "        if debug:\n",
    "            for a in  items_sorted[:n_best]:\n",
    "                print((a, item_similarities.get(a, 0)))\n",
    "        return items_sorted[:n_best]\n",
    "    return recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend_dummy(user, n_best = 10):\n",
    "    item_similarities = []\n",
    "    for item in all_items:\n",
    "        #пропустим те фильмы, которые пользователь уже просмотрел, если нас об этом попросили\n",
    "        if item in user_to_items[user]: continue\n",
    "        item_similarities.append(item)\n",
    "         \n",
    "    random.shuffle(item_similarities)\n",
    "    #вернём n_best наиболее пригодных\n",
    "    #print(items_sorted[:n_best])\n",
    "    return item_similarities[:n_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend_mk0(user, n_best = 10):\n",
    "    user_items = user_to_items[user]\n",
    "    \n",
    "    neighborhood = Counter()\n",
    "    for item in user_items:\n",
    "        neighborhood.update(item_to_users[item])\n",
    "    \n",
    "    #словарь {фильм -> пригодность фильма пользователю}\n",
    "    item_similarities = {}\n",
    "    \n",
    "    for item in all_items:\n",
    "        if item in user_items: continue\n",
    "        item_users = item_to_users[item]\n",
    "        if len(item_users) == 0: continue\n",
    "        \n",
    "        n_common_users = sum(neighborhood[user] for user in item_users)\n",
    "        similarity = float(n_common_users) / sqrt(len(item_users))\n",
    "        item_similarities[item] = similarity\n",
    "    \n",
    "    items_sorted = sorted(all_items, key = lambda x: item_similarities.get(x, 0),reverse = True)\n",
    "    \n",
    "    return items_sorted[:n_best]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data precalc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_to_int = dict()\n",
    "for i, user in enumerate(all_users):\n",
    "    user_to_int[user] = i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#User_features is a dict of user's features from the same space as for films\n",
    "user_features = defaultdict(lambda:defaultdict(lambda:0))\n",
    "for user in list(user_to_items.keys())[0:]:\n",
    "    for item in user_to_items[user]:\n",
    "        if item in films:\n",
    "            for feature, value in films[item].items():    \n",
    "                user_features[user][feature]+=value\n",
    "    for f in user_features[user]:\n",
    "        user_features[user][f]/=len(user_to_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_size = 2*small_features_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(user=None, item=None, X_sample = np.zeros(features_size, dtype='float32')):\n",
    "    'Returns 1 sample for specified user and film'\n",
    "    if user is not None:\n",
    "        for f in user_features[user].keys()&small_features:\n",
    "            X_sample[feature_to_i[f]] = user_features[user][f]\n",
    "        if item is not None:\n",
    "            X_sample[2*small_features_size+0] = user_film_mk2(user, item)\n",
    "    if item is not None and item in films:\n",
    "        for f in films[item].keys()&small_features:\n",
    "            X_sample[small_features_size+feature_to_i[f]] = films[item][f]\n",
    "    return X_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def user_film_mk1(user, item, debug=False):\n",
    "    #print(len(user_features[user]))\n",
    "    cursum = 0\n",
    "    curcnt = 0\n",
    "    if item in films:\n",
    "        for feature, value in films[item].items():\n",
    "            if type(value) is int:\n",
    "                curcnt+=1    \n",
    "                cursum+=value*user_features[user][feature]\n",
    "    if curcnt==0:\n",
    "        curcnt+=1\n",
    "    #cursum/=curcnt\n",
    "    if debug:\n",
    "        print(cursum)\n",
    "    return cursum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_random_samples(X_size = 100, use_test=False):\n",
    "    'Generate samples randomly from any part of dataset'\n",
    "    X = np.zeros((X_size, features_size), dtype='float32')\n",
    "    Y = np.zeros(X_size, dtype='int8')\n",
    "    if use_test:\n",
    "        local_users_list = test_users_list\n",
    "        local_user_to_items = test_user_to_items\n",
    "    else:\n",
    "        local_users_list = users_list\n",
    "        local_user_to_items = user_to_items\n",
    "    for i in range(X_size):\n",
    "        film =''\n",
    "        while film not in films: \n",
    "            result = random.random()\n",
    "            if result > 0.50:\n",
    "                result = 1\n",
    "            else:\n",
    "                result = 0\n",
    "            user = random.choice(local_users_list)\n",
    "            if result==0:\n",
    "                film = random.choice(all_items_list)\n",
    "                if film in local_user_to_items[user]:\n",
    "                    result = 1\n",
    "            else:\n",
    "                film = random.choice(list(local_user_to_items[user]|{''}))\n",
    "        X[i] = extract_features(user, film)\n",
    "        Y[i] = result\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_samples(X_true = 100,  use_test=False):\n",
    "    'Generate samples from begining one-by-one'\n",
    "    X_fake=X_true\n",
    "    X = np.zeros((X_true+X_fake, features_size), dtype='float32')\n",
    "    Y = np.zeros(X_true+X_fake, dtype='float32')\n",
    "    if use_test:\n",
    "        local_users_list = test_users_list\n",
    "        local_user_to_items = test_user_to_items\n",
    "        local_edges = test_edges \n",
    "    else: \n",
    "        local_users_list = users_list\n",
    "        local_user_to_items = user_to_items\n",
    "        local_edges = edges\n",
    "    for i, (user, film)  in enumerate(local_edges[:X_true]):\n",
    "        X[i] = extract_features(user, film)\n",
    "        Y[i] = 1\n",
    "    for i in range(X_true, X_true+X_fake):\n",
    "        user = random.choice(local_users_list)\n",
    "        film = random.choice(all_items_list)\n",
    "        X[i] = extract_features(user, film)\n",
    "        if film in local_user_to_items[user]:\n",
    "            Y[i] = 1\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = generate_random_samples(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_film_mk3(user, item, debug=False):\n",
    "    X = np.zeros((1, features_size), dtype='float32')\n",
    "    X[0] = extract_features(user, item)\n",
    "    probs = rf.predict(X)\n",
    "    if debug:\n",
    "        print(probs)\n",
    "    return probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend_mk3_manual(user, n_best = 10, debug=False):\n",
    "    user_items = user_to_items[user]\n",
    "\n",
    "    item_similarities = {}\n",
    "    cur_items = [item for item in all_items if item not in user_items and len(item_to_users[item])>0]\n",
    "    \n",
    "    X = np.zeros((len(cur_items), features_size), dtype='float32')\n",
    "    X_user = extract_features(user)\n",
    "    for i, item in enumerate(cur_items):\n",
    "        X[i] = extract_features(user=None, item=item, X_sample=X_user)\n",
    "    \n",
    "    probs =rf.predict(X)\n",
    "    item_similarities = {key: prob for key, prob in zip(cur_items, probs)}\n",
    "    items_sorted = sorted(all_items, key = lambda x: item_similarities.get(x, 0),reverse = True)\n",
    "    if debug:\n",
    "        print(X)\n",
    "        for a in  items_sorted[:n_best]:\n",
    "            print((a, item_similarities.get(a, 0)))\n",
    "    return items_sorted[:n_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = generate_random_samples(100, use_test=False)\n",
    "print(\"Train score: %s\" %(rf.score(X, Y),))\n",
    "X, Y = generate_random_samples(100, use_test=True)\n",
    "print(\"Test score: %s\" %(rf.score(X, Y),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = generate_random_samples(10, use_test=True)\n",
    "X[0], rf.predict(X), Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasagne solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_X = T.matrix(\"input X\")\n",
    "input_shape = [None, features_size]\n",
    "\n",
    "target_y = T.matrix(\"target Y integer\",dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "layer = lasagne.layers.BatchNormLayer(layer)\n",
    "\n",
    "layer = lasagne.layers.DenseLayer(layer,num_units=features_size,\n",
    "                                   nonlinearity = lasagne.nonlinearities.linear,\n",
    "                                   name = \"hidden_dense_layer\")\n",
    "layer = lasagne.layers.DenseLayer(layer,num_units=features_size,\n",
    "                                   nonlinearity = lasagne.nonlinearities.linear,\n",
    "                                   name = \"hidden_dense_layer\")\n",
    "layer = lasagne.layers.DenseLayer(layer,num_units=features_size,\n",
    "                                   nonlinearity = lasagne.nonlinearities.sigmoid,\n",
    "                                   name = \"hidden_dense_layer\")\n",
    "\n",
    "layer = lasagne.layers.DropoutLayer(layer, p=0.5)\n",
    "\n",
    "layer = lasagne.layers.DenseLayer(layer,num_units=features_size,\n",
    "                                   nonlinearity = lasagne.nonlinearities.sigmoid,\n",
    "                                   name = \"hidden_dense_layer\")\n",
    "dense_output = lasagne.layers.DenseLayer(layer,num_units = 1 ,\n",
    "                                        nonlinearity = lasagne.nonlinearities.sigmoid,\n",
    "                                        name='output', b=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_predicted = lasagne.layers.get_output(dense_output)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output, trainable=True)\n",
    "\n",
    "loss = lasagne.objectives.squared_error(y_predicted, target_y)\n",
    "loss = lasagne.objectives.aggregate(loss, mode = 'mean') #use mean squared_error\n",
    "\n",
    "\n",
    "updates = lasagne.updates.adadelta(loss, all_weights,learning_rate=0.01)\n",
    "train_fun = theano.function([input_X,target_y],loss,updates= updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_predicted_clear = lasagne.layers.get_output(dense_output, deterministic=True)\n",
    "predict = theano.function([input_X], y_predicted_clear)\n",
    "\n",
    "\n",
    "loss_clear = lasagne.objectives.squared_error(y_predicted_clear, target_y)\n",
    "loss_clear = lasagne.objectives.aggregate(loss_clear, mode = 'mean') #use mean squared_error\n",
    "loss_fun = theano.function([input_X,target_y],loss_clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 3 #amount of passes through the data\n",
    "\n",
    "batch_size = 1000 #number of samples processed at each function call\n",
    "num_batches = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in (generate_samples(batch_size, use_test=False) for _ in range(num_batches)):\n",
    "        inputs, targets = batch\n",
    "        targets = targets.reshape((-1, 1))\n",
    "        train_err_batch = train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_batches = 0\n",
    "    for batch in (generate_samples(batch_size, use_test=True) for _ in range(num_batches)):\n",
    "        inputs, targets = batch\n",
    "        targets = targets.reshape((-1, 1))\n",
    "        val_err += loss_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  train loss: %s\" % (train_err / train_batches, ))\n",
    "    print(\"  test loss: %s\" % (val_err / val_batches, ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best\n",
    "test loss: 0.15436154014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = generate_random_samples(10, use_test=True)\n",
    "predict(X), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend_mk4_manual(user, n_best = 10, debug=False):\n",
    "    user_items = user_to_items[user]\n",
    "\n",
    "    item_similarities = {}\n",
    "    cur_items = [item for item in all_items if item not in user_items and len(item_to_users[item])>0]\n",
    "    \n",
    "    X = np.zeros((len(cur_items), features_size), dtype='float32')\n",
    "    X_user = extract_features(user)\n",
    "    for i, item in enumerate(cur_items):\n",
    "        X[i] = extract_features(user=None, item=item, X_sample=X_user)\n",
    "    \n",
    "    probs = predict(X)\n",
    "    item_similarities = {key: prob for key, prob in zip(cur_items, probs)}\n",
    "    items_sorted = sorted(all_items, key = lambda x: item_similarities.get(x, 0),reverse = True)\n",
    "    if debug:\n",
    "        print(X)\n",
    "        for a in  items_sorted[:n_best]:\n",
    "            print((a, item_similarities.get(a, 0)))\n",
    "    return items_sorted[:n_best]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow solution\n",
    "This solution is not finished (model is working, but recommend function is absent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 1000\n",
    "total_batch = 200\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, features_size]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 1]) \n",
    "\n",
    "W = tf.Variable(tf.zeros([features_size, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "pred = tf.nn.tanh(tf.matmul(x, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(tf.squared_difference(y, pred))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = generate_random_samples(batch_size)\n",
    "        batch_ys = batch_ys.reshape((-1, 1))\n",
    "        # Fit training using batch data\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                      y: batch_ys})\n",
    "        # Compute average loss\n",
    "        avg_cost += c / total_batch\n",
    "    # Display logs per epoch step\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"%s\" % (avg_cost))\n",
    "        x_test, y_test =   generate_random_samples(batch_size, use_test=True)\n",
    "        y_test = y_test.reshape((-1, 1))\n",
    "        print (\"Train cost:\", cost.eval({x: x_test, y: y_test}, session=sess))\n",
    "\n",
    "print (\"Optimization Finished!\")\n",
    "\n",
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "x_test, y_test =   generate_random_samples(batch_size, use_test=True)\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "print (\"Train cost:\", cost.eval({x: x_test, y: y_test}, session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = generate_random_samples(10, use_test=False)\n",
    "sess.run([pred], feed_dict={x: X}), Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generic_recommend(user_film_mk1)('1a337111f63f6a7f86cebf6b2ad3d732', debug=True)\n",
    "user_film_mk1('1a337111f63f6a7f86cebf6b2ad3d732', 'c745ed11b94ed93f3008897c75240de3', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generic_recommend(user_film_mk2)('1a337111f63f6a7f86cebf6b2ad3d732', debug=True)\n",
    "user_film_mk2('1a337111f63f6a7f86cebf6b2ad3d732', 'c745ed11b94ed93f3008897c75240de3', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_film_mk3('26052b20aa96ed8d803dbfe4e9497192', '45ea2aa2143effcb6575daf0143e31b4', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommend_mk4_manual('a18f526db904f8f2ee4c8d178bfc818c', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(test_user_to_items.items())[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality check - errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Random forest score (less - better)\n",
    "X, Y = generate_random_samples(100, use_test=False)\n",
    "print(\"Train score: %s\" %(rf.score(X, Y),))\n",
    "X, Y = generate_random_samples(100, use_test=True)\n",
    "print(\"Test score: %s\" %(rf.score(X, Y),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lasagne error\n",
    "inputs, targets = generate_random_samples(100, use_test=False)\n",
    "targets = targets.reshape((-1, 1))\n",
    "print(\"Train score: %s\" % (loss_fun(inputs, targets),)) \n",
    "inputs, targets = generate_samples(100, use_test=True)\n",
    "targets = targets.reshape((-1, 1))\n",
    "print(\"Test score: %s\" % (loss_fun(inputs, targets),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Quality check - map@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#сколько рекоммендаций рассматриваем\n",
    "def check_quality(function, K = 10, max_n_users = len(test_user_to_items)):\n",
    "    APatK_per_user = []\n",
    "    user_list = list(test_user_to_items.keys())[:max_n_users]\n",
    "    \n",
    "    for i, user in enumerate(user_list):\n",
    "        #фильмы, которые пользователю на самом деле нравятся\n",
    "        test_items = test_user_to_items[user]\n",
    "\n",
    "        #Выдать топ-K рекоммендаций\n",
    "        recommendation_list = function(user,n_best=K)\n",
    "        #Посчитать ap@k\n",
    "        user_APatK = apk(test_items, recommendation_list,k=K)\n",
    "\n",
    "        #и сложить в коробку\n",
    "        APatK_per_user.append(user_APatK)\n",
    "\n",
    "        #Progress bar\n",
    "        if i % 100 ==0:\n",
    "            print(i,'/',max_n_users)\n",
    "\n",
    "        if i > max_n_users:\n",
    "            break\n",
    "\n",
    "    print('AP@{} = {}'.format(K, sum(APatK_per_user)/len(APatK_per_user)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_quality(recommend_dummy,100,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_quality(recommend_mk0,100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_quality(generic_recommend(user_film_mk1),100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_quality(generic_recommend(user_film_mk2),200,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_quality(recommend_mk3_manual,100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_quality(recommend_mk4_manual,100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

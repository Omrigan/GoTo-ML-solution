{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import json\n",
    "from metrics import apk\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Словари для основной выборки\n",
    "user_to_items = defaultdict(set)\n",
    "item_to_users = defaultdict(set)\n",
    "edges = []\n",
    "#Словари для тестовой выборки\n",
    "test_user_to_items = defaultdict(set)\n",
    "test_item_to_users = defaultdict(set)\n",
    "test_edges = []\n",
    "\n",
    "with open(\"data/train_likes.csv\") as datafile:\n",
    "    for like in csv.DictReader(datafile):\n",
    "        # Кидаем монетку. В зависимости от результата кладём в обучение или тест\n",
    "        if random.random() < 0.90:\n",
    "            user_to_items[like['user_id']].add(like['item_id'])\n",
    "            item_to_users[like['item_id']].add(like['user_id'])\n",
    "            edges.append((like['user_id'], like['item_id']))\n",
    "        else:\n",
    "            test_user_to_items[like['user_id']].add(like['item_id'])\n",
    "            test_item_to_users[like['item_id']].add(like['user_id'])\n",
    "            test_edges.append((like['user_id'], like['item_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_items = set(item_to_users.keys()) | set(test_item_to_users.keys())\n",
    "all_users = set(user_to_items.keys()) | set(test_user_to_items.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_list = list(user_to_items.keys())\n",
    "test_users_list = list(test_user_to_items.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_to_i = {user: i for i, user in enumerate(all_users)}\n",
    "item_to_i = {item: i for i, item in enumerate(all_items)}\n",
    "all_users_list = list(all_users)\n",
    "all_items_list = list(all_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix = sp.sparse.lil_matrix((len(all_users), len(all_items)))\n",
    "test_matrix = sp.sparse.lil_matrix((len(all_users), len(all_items)))\n",
    "for user, items in user_to_items.items():\n",
    "    for item in items:\n",
    "        matrix[user_to_i[user], item_to_i[item]] = True\n",
    "for user, items in test_user_to_items.items():\n",
    "    for item in items:\n",
    "        test_matrix[user_to_i[user], item_to_i[item]] = True\n",
    "matrix = matrix.tocsr()\n",
    "test_matrix = test_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<55863x23891 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 99582 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u, s, vt = sp.sparse.linalg.svds(matrix.astype(np.float32), k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55863, 100), (100,), (100, 23891))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape, s.shape, vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "films = json.load(open('data/items.json'))\n",
    "films = {a['id']:a for a in films}\n",
    "for a in films.values():\n",
    "    del a['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in films.values():\n",
    "    if 'genre' in f:\n",
    "        f[f['genre']] = 1\n",
    "        del f['genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = Counter()\n",
    "for f in films.values():\n",
    "    features.update(set(f.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('year', 138078),\n",
       " ('duration', 138078),\n",
       " (0, 34128),\n",
       " (3, 32154),\n",
       " ('f_114306', 30650),\n",
       " (1, 28635),\n",
       " ('f_79251', 16647),\n",
       " ('f_117573', 15295),\n",
       " ('f_138980', 15215),\n",
       " (5, 15008),\n",
       " ('f_64513', 14020),\n",
       " ('f_205162', 10693),\n",
       " ('f_122038', 10436),\n",
       " ('f_84602', 8377),\n",
       " ('f_68894', 8348),\n",
       " ('f_127793', 7946),\n",
       " ('f_72071', 7866),\n",
       " (4, 7386),\n",
       " ('f_210900', 7085),\n",
       " ('f_130202', 6371)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_features_size = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_features = set([_[0] for _ in features.most_common(small_features_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_to_i = {feature: i for i, feature in enumerate(small_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 3: 3,\n",
       " 'f_68894': 6,\n",
       " 5: 5,\n",
       " 'f_84602': 2,\n",
       " 'f_79251': 9,\n",
       " 'f_114306': 10,\n",
       " 'f_117573': 7,\n",
       " 'year': 13,\n",
       " 'f_210900': 14,\n",
       " 'f_205162': 19,\n",
       " 'f_138980': 4,\n",
       " 'f_64513': 11,\n",
       " 'f_72071': 15,\n",
       " 4: 8,\n",
       " 'f_122038': 16,\n",
       " 'f_127793': 17,\n",
       " 'duration': 18,\n",
       " 'f_130202': 12}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_to_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фильтрация пользователей\n",
    "* Значительная часть пользователей имеет всего 1-2 просмотра. При всём желании, рекоммендовать им что-либо осмысленное при помощи рассматриваемого здесь метода мы вряд ли сможем. Для простоты вычислений, удалим их из выборки.\n",
    "* Важно понимать, что качество на оставшихся пользователях скорее всего будет выше, чем на первоначальной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_items_per_user = 2\n",
    "from copy import copy\n",
    "for user in copy(test_user_to_items).keys():\n",
    "    \n",
    "    n_items_per_user = len(user_to_items[user]) + len(test_user_to_items[user])\n",
    "    \n",
    "    if n_items_per_user <= min_items_per_user:\n",
    "        del user_to_items[user]\n",
    "        del test_user_to_items[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рекоммендующая функция\n",
    "Позволим себе немного вольности: наша функция будет возвращать не вероятности, а список фильмов в порядке убывания \"рекомендованности\".\n",
    "\n",
    "* Рекоммендованность фильма item пользователю user посчитаем так:\n",
    "  * Для каждого фильма, полайканного пользователем user, найдём других людей, которым понравился фильм.\n",
    "  * Сложим всех таких \"друзей по лайкам\" вместе и назовём соседями (__neighborhood__) пользователя.\n",
    "  * Для фильма item узнаем его аудиторию - множество пользователей, которые его лайкнули\n",
    "  * Пригодность фильма пользователю - то, насколько \"друзьям по лайкам\" пользователя нравится этот фильм.\n",
    "\n",
    "Для примера, будем использовать косинусную меру расстояния\n",
    "  \n",
    "$ cos(u_{film}, u_{neighborhood}) = $ =$ u_{film} \\cdot u_{neighborhood} \\over |u_{film}| |u_{neighborhood}| $\n",
    "\n",
    "\n",
    "$u_{neighborhood}$ зависит только от пользователя, но не от фильма, поэтому при сравнении фильмов по пригодности для одного пользователя, его можно исключить из формулы для простоты вычислений.\n",
    "\n",
    "$ similarity(u_{film}, u_{neighborhood}) = $ $  u_{film} \\cdot u_{neighborhood} \\over |u_{film}| $\n",
    "  \n",
    "  \n",
    "Распишем формулу подробно:\n",
    "\n",
    "$ similarity(u_{film}, u_{neighborhood}) = $ $ \\sum _{u_i} [u_i \\in u_{film}] \\cdot [u_i \\in u_{neighborhood}] \\over |u_{film}|  $\n",
    "\n",
    "* u_i - очередной пользователь (в цикле по всем пользователям)\n",
    "  \n",
    "Выражение $[u_i \\in u_{neighborhood}]$ здесь означает \"сколько раз очередной пользователь входит в множество друзей по лайкам\"\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from collections import Counter\n",
    "\n",
    "def recommend(user, n_best = 10):\n",
    "    user_items = user_to_items[user]\n",
    "    \n",
    "    neighborhood = Counter()\n",
    "    for item in user_items:\n",
    "        neighborhood.update(item_to_users[item])\n",
    "    \n",
    "    #словарь {фильм -> пригодность фильма пользователю}\n",
    "    item_similarities = {}\n",
    "    \n",
    "    for item in all_items:\n",
    "        if item in user_items: continue\n",
    "        item_users = item_to_users[item]\n",
    "        if len(item_users) == 0: continue\n",
    "        \n",
    "        n_common_users = sum(neighborhood[user] for user in item_users)\n",
    "        similarity = float(n_common_users) / sqrt(len(item_users))\n",
    "        item_similarities[item] = similarity\n",
    "    \n",
    "    items_sorted = sorted(all_items, key = lambda x: item_similarities.get(x, 0),reverse = True)\n",
    "    \n",
    "    return items_sorted[:n_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_to_int = dict()\n",
    "for i, user in enumerate(all_users):\n",
    "    user_to_int[user] = i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Making dataset\n",
    "user_features = defaultdict(lambda:defaultdict(lambda:0))\n",
    "for user in list(user_to_items.keys())[0:]:\n",
    "    for item in user_to_items[user]:\n",
    "        if item in films:\n",
    "            for feature, value in films[item].items():    \n",
    "                user_features[user][feature]+=value\n",
    "    for f in user_features[user]:\n",
    "        user_features[user][f]/=len(user_to_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges[0] in test_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_size = 2*small_features_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(user=None, item=None, X_sample = np.zeros(features_size, dtype='float32')):\n",
    "    if user is not None:\n",
    "        for f in user_features[user].keys()&small_features:\n",
    "            X_sample[feature_to_i[f]] = user_features[user][f]\n",
    "        if item is not None:\n",
    "            X_sample[2*small_features_size+0] = user_film_mk2(user, item)\n",
    "    if item is not None and item in films:\n",
    "        for f in films[item].keys()&small_features:\n",
    "            X_sample[small_features_size+feature_to_i[f]] = films[item][f]\n",
    "    \n",
    "    return X_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100068"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_random_samples(X_size = 100, use_test=False):\n",
    "    X = np.zeros((X_size, features_size), dtype='float32')\n",
    "    Y = np.zeros(X_size, dtype='int8')\n",
    "    if use_test:\n",
    "        local_users_list = test_users_list\n",
    "        local_user_to_items = test_user_to_items\n",
    "    else:\n",
    "        local_users_list = users_list\n",
    "        local_user_to_items = user_to_items\n",
    "    for i in range(X_size):\n",
    "        film =''\n",
    "        while film not in films: \n",
    "            result = random.random()\n",
    "            if result > 0.50:\n",
    "                result = 1\n",
    "            else:\n",
    "                result = 0\n",
    "            user = random.choice(local_users_list)\n",
    "            if result==0:\n",
    "                film = random.choice(all_items_list)\n",
    "                if film in local_user_to_items[user]:\n",
    "                    result = 1\n",
    "            else:\n",
    "                film = random.choice(list(local_user_to_items[user]|{''}))\n",
    "        X[i] = extract_features(user, film)\n",
    "        Y[i] = result\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_samples(X_true = 100,  use_test=False):\n",
    "    X_fake=X_true\n",
    "    X = np.zeros((X_true+X_fake, features_size), dtype='float32')\n",
    "    Y = np.zeros(X_true+X_fake, dtype='float32')\n",
    "    if use_test:\n",
    "        local_users_list = test_users_list\n",
    "        local_user_to_items = test_user_to_items\n",
    "        local_edges = test_edges \n",
    "    else: \n",
    "        local_users_list = users_list\n",
    "        local_user_to_items = user_to_items\n",
    "        local_edges = edges\n",
    "    for i, (user, film)  in enumerate(local_edges[:X_true]):\n",
    "        X[i] = extract_features(user, film)\n",
    "        Y[i] = 1\n",
    "    for i in range(X_true, X_true+X_fake):\n",
    "        user = random.choice(local_users_list)\n",
    "        film = random.choice(all_items_list)\n",
    "        X[i] = extract_features(user, film)\n",
    "        if film in local_user_to_items[user]:\n",
    "            Y[i] = 1\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = generate_random_samples(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.878675890313\n",
      "Test score: 0.202766900421\n"
     ]
    }
   ],
   "source": [
    "X, Y = generate_random_samples(100, use_test=False)\n",
    "print(\"Train score: %s\" %(rf.score(X, Y),))\n",
    "X, Y = generate_random_samples(100, use_test=True)\n",
    "print(\"Test score: %s\" %(rf.score(X, Y),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_X = T.matrix(\"input X\")\n",
    "input_shape = [None, features_size]\n",
    "\n",
    "target_y = T.matrix(\"target Y integer\",dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Input layer (auxilary)\n",
    "layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "#fully connected layer, that takes input layer and applies 50 neurons to it.\n",
    "# nonlinearity here is sigmoid as in logistic regression\n",
    "# you can give a name to each layer (optional)\n",
    "layer = lasagne.layers.DenseLayer(layer,num_units=50,\n",
    "                                   nonlinearity = lasagne.nonlinearities.tanh,\n",
    "                                   name = \"hidden_dense_layer\")\n",
    "\n",
    "#fully connected output layer that takes dense_1 as input and has 10 neurons (1 for each digit)\n",
    "#We use softmax nonlinearity to make probabilities add up to 1\n",
    "dense_output = lasagne.layers.DenseLayer(layer,num_units = 1 ,\n",
    "                                        nonlinearity = lasagne.nonlinearities.rectify,\n",
    "                                        name='output', b=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hidden_dense_layer.W, hidden_dense_layer.b, output.W]\n"
     ]
    }
   ],
   "source": [
    "#network prediction (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(dense_output)\n",
    "#all network weights (shared variables)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output, trainable=True)\n",
    "print (all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = lasagne.objectives.squared_error(y_predicted, target_y)\n",
    "loss = lasagne.objectives.aggregate(loss, mode = 'mean')\n",
    "\n",
    "accuracy = lasagne.objectives.squared_error(y_predicted, target_y)\n",
    "accuracy = lasagne.objectives.aggregate(accuracy, mode = 'mean')\n",
    "\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.sgd(loss, all_weights,learning_rate=0.1)\n",
    "\n",
    "#function that computes loss and updates weights\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#function that just computes accuracy\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1000 took 12.404s\n",
      "  training loss (in-iteration):\t\t0.500035\n",
      "  train accuracy:\t\t0.500035 \n",
      "  validation accuracy:\t\t0.5000150000000001 \n",
      "Epoch 2 of 1000 took 12.282s\n",
      "  training loss (in-iteration):\t\t0.500040\n",
      "  train accuracy:\t\t0.50004 \n",
      "  validation accuracy:\t\t0.5000100000000001 \n",
      "Epoch 3 of 1000 took 12.088s\n",
      "  training loss (in-iteration):\t\t0.500055\n",
      "  train accuracy:\t\t0.5000550000000001 \n",
      "  validation accuracy:\t\t0.5000100000000001 \n",
      "Epoch 4 of 1000 took 12.794s\n",
      "  training loss (in-iteration):\t\t0.500040\n",
      "  train accuracy:\t\t0.50004 \n",
      "  validation accuracy:\t\t0.5 \n",
      "Epoch 5 of 1000 took 13.136s\n",
      "  training loss (in-iteration):\t\t0.500020\n",
      "  train accuracy:\t\t0.5000200000000001 \n",
      "  validation accuracy:\t\t0.50001 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-8323a6db6020>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mval_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgenerate_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-8323a6db6020>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mval_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgenerate_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-86-ffbbaab0f2b2>\u001b[0m in \u001b[0;36mgenerate_samples\u001b[1;34m(X_true, use_test)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0muser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_users_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mfilm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_items_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlocal_user_to_items\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 1000 #amount of passes through the data\n",
    "\n",
    "batch_size = 1000 #number of samples processed at each function call\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in (generate_samples(batch_size, use_test=False) for _ in range(100)):\n",
    "        inputs, targets = batch\n",
    "        targets = targets.reshape((-1, 1))\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in (generate_samples(batch_size, use_test=True) for _ in range(100)):\n",
    "        inputs, targets = batch\n",
    "        targets = targets.reshape((-1, 1))\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{} \".format(\n",
    "        train_acc / train_batches ))\n",
    "    print(\"  validation accuracy:\\t\\t{} \".format(\n",
    "        val_acc / val_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]), array([0, 0, 1, 1, 0, 1, 1, 1, 0, 0], dtype=int8))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = generate_random_samples(10, use_test=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([(hidden_dense_layer.W, Elemwise{add,no_inplace}.0), (hidden_dense_layer.b, Elemwise{add,no_inplace}.0), (output.W, Elemwise{add,no_inplace}.0), (<TensorType(float64, matrix)>, Elemwise{sub,no_inplace}.0), (<TensorType(float64, vector)>, Elemwise{sub,no_inplace}.0), (<TensorType(float64, matrix)>, Elemwise{sub,no_inplace}.0)])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.98906996e-05,   0.00000000e+00,   5.98360493e-05,\n",
       "          0.00000000e+00,   0.00000000e+00,   3.98906996e-05,\n",
       "          0.00000000e+00,   7.97813991e-05,   1.99453498e-05,\n",
       "          9.97267489e-05,   3.98906996e-05,   3.98906996e-05,\n",
       "         -3.40250190e-05,   1.99453498e-05,   1.99453498e-05,\n",
       "          8.33385002e-06,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.99453498e-05,   1.99453498e-05,   1.00000000e+00,\n",
       "          1.00000000e+00,   2.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          2.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   2.00000000e+00,  -6.53573573e-01,\n",
       "          2.00000000e+00,   1.00000000e+00,   4.65432197e-01,\n",
       "          1.00000000e+00,   1.00000000e+00,   2.00000000e+00,\n",
       "          1.00000000e+00,   1.09823048e-03,   0.00000000e+00], dtype=float32),\n",
       " array([ 0.74074074,  0.09090909,  0.10869565,  0.06      ,  0.12727273,\n",
       "         0.12962963,  0.11111111,  0.12121212,  0.26470588,  0.14925373]),\n",
       " array([1, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=int8))"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = generate_random_samples(10, use_test=True)\n",
    "X[0], rf.predict(X), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_film_mk3(user, item, debug=False):\n",
    "    X = np.zeros((1, features_size), dtype='float32')\n",
    "    X[0] = extract_features(user, item)\n",
    "    probs = rf.predict(X)\n",
    "    if debug:\n",
    "        print(probs)\n",
    "    return probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_film_mk1(user, item, debug=False):\n",
    "    #print(len(user_features[user]))\n",
    "    cursum = 0\n",
    "    curcnt = 0\n",
    "    if item in films:\n",
    "        for feature, value in films[item].items():\n",
    "            if type(value) is int:\n",
    "                curcnt+=1    \n",
    "                cursum+=value*user_features[user][feature]\n",
    "    if curcnt==0:\n",
    "        curcnt+=1\n",
    "    #cursum/=curcnt\n",
    "    if debug:\n",
    "        print(cursum)\n",
    "    return cursum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_film_mk2(user, item, debug=False):\n",
    "    ui = user_to_i[user]\n",
    "    ii = item_to_i[item]\n",
    "    if debug:\n",
    "        print(ui, ii)\n",
    "    a = np.dot(u[ui,:] * s, vt[:,ii])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_film_mk3('26052b20aa96ed8d803dbfe4e9497192', '45ea2aa2143effcb6575daf0143e31b4', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recommend_mk3_manual(user, n_best = 10, debug=False):\n",
    "    user_items = user_to_items[user]\n",
    "\n",
    "    item_similarities = {}\n",
    "    cur_items = [item for item in all_items if item not in user_items and len(item_to_users[item])>0]\n",
    "    \n",
    "    X = np.zeros((len(cur_items), features_size), dtype='float32')\n",
    "    X_user = extract_features(user)\n",
    "    for i, item in enumerate(cur_items):\n",
    "        X[i] = extract_features(user=None, item=item, X_sample=X_user)\n",
    "    \n",
    "    probs =rf.predict(X)\n",
    "    item_similarities = {key: prob for key, prob in zip(cur_items, probs)}\n",
    "    items_sorted = sorted(all_items, key = lambda x: item_similarities.get(x, 0),reverse = True)\n",
    "    if debug:\n",
    "        print(X)\n",
    "        for a in  items_sorted[:n_best]:\n",
    "            print((a, item_similarities.get(a, 0)))\n",
    "    return items_sorted[:n_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generic_recommend(user_flim_function):\n",
    "    def recommend(user, n_best = 10, debug=False):\n",
    "        user_items = user_to_items[user]\n",
    "\n",
    "        item_similarities = {}\n",
    "        for item in all_items:\n",
    "            if item in user_items: continue\n",
    "            item_users = item_to_users[item]\n",
    "            if len(item_users) == 0: continue\n",
    "\n",
    "            item_similarities[item] = user_flim_function(user, item)\n",
    "\n",
    "        items_sorted = sorted(all_items, key = lambda x: item_similarities.get(x, 0),reverse = True)\n",
    "        if debug:\n",
    "            for a in  items_sorted[:n_best]:\n",
    "                print((a, item_similarities.get(a, 0)))\n",
    "        return items_sorted[:n_best]\n",
    "    return recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('7682d6641caf183ffcf2616c9bb1e434', 0.08739379)\n",
      "('b5e424608d041f38c2fd6125b3bcd40f', 0.062321573)\n",
      "('006a602774c79d476cdbd5dff7dd24f1', 0.047887065)\n",
      "('e9caaa12a277abf87ba07b84e09a4170', 0.045560129)\n",
      "('aa5f2ca699da42e467e550f9f071fb3f', 0.044249389)\n",
      "('bcee3a68dbfb6b4ea97ac53ceb1d3287', 0.044186898)\n",
      "('e8047b4262e676d28f50816ac3fde1ca', 0.042720947)\n",
      "('77c7998b3d3d41f3fc3aec786b4015ac', 0.038098648)\n",
      "('1da6f84598fbeea050da49fe37297932', 0.037306551)\n",
      "('2dc12b93072f94bca9438902bddc36c4', 0.035170436)\n",
      "10976 651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0024645075"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_recommend(user_film_mk2)('1a337111f63f6a7f86cebf6b2ad3d732', debug=True)\n",
    "user_film_mk2('1a337111f63f6a7f86cebf6b2ad3d732', 'c745ed11b94ed93f3008897c75240de3', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e13df82b3252eaeefc0c13599695f9c7', 0.000658196541476355)\n",
      "('634e622e5c957c2cd8ddb98ca75ef937', 0.0005584697927678163)\n",
      "('8faa6327a2ca2eb92e738ffc204d35c0', 0.0005584697927678163)\n",
      "('64b6536649e20bd72d7183ec717b6c86', 0.0005385244430261086)\n",
      "('5c6d1862dc1e82f340962b308999e603', 0.0005185790932844008)\n",
      "('bcee3a68dbfb6b4ea97ac53ceb1d3287', 0.0005185790932844007)\n",
      "('ef786a839c609561b76ce03768eace80', 0.0005185790932844007)\n",
      "('ff987a60d68f5bbbefe97f8c5062711a', 0.0004986337435426931)\n",
      "('753bce7ac492e6c95b07b85f8877cf5a', 0.0004786883938009853)\n",
      "('7b0f1b0815292c0a550fabd5eeadc011', 0.00045874304405927767)\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_recommend(user_film_mk1)('1a337111f63f6a7f86cebf6b2ad3d732', debug=True)\n",
    "user_film_mk1('1a337111f63f6a7f86cebf6b2ad3d732', 'c745ed11b94ed93f3008897c75240de3', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.99453498e-05   3.39070946e-04   3.98906996e-05 ...,   1.00000000e+00\n",
      "    8.16810257e-07   0.00000000e+00]\n",
      " [  1.99453498e-05   3.39070946e-04   3.98906996e-05 ...,   1.00000000e+00\n",
      "    8.16810257e-07   0.00000000e+00]\n",
      " [  1.99453498e-05   3.39070946e-04   3.98906996e-05 ...,   1.00000000e+00\n",
      "    8.16810257e-07   0.00000000e+00]\n",
      " ..., \n",
      " [  1.99453498e-05   3.39070946e-04   3.98906996e-05 ...,   1.00000000e+00\n",
      "    8.16810257e-07   0.00000000e+00]\n",
      " [  1.99453498e-05   3.39070946e-04   3.98906996e-05 ...,   1.00000000e+00\n",
      "    8.16810257e-07   0.00000000e+00]\n",
      " [  1.99453498e-05   3.39070946e-04   3.98906996e-05 ...,   1.00000000e+00\n",
      "    8.16810257e-07   0.00000000e+00]]\n",
      "('1df5a0fbce075d47107feeb8d65a138b', 0.27777777777777779)\n",
      "('7db05f44445f307fd8157d7bc5e4db51', 0.27777777777777779)\n",
      "('47c701ff962c505d385bfd251f061c42', 0.27777777777777779)\n",
      "('68d0fcae54011b2f7d96f82ef4b0af2c', 0.27777777777777779)\n",
      "('bd03a2df7386c06c1f7b3a3a8661ece4', 0.27777777777777779)\n",
      "('4740836db6ce4b2144d81fd6a9a796c9', 0.27777777777777779)\n",
      "('4bef82fe1a89bc190e1d2b0e2c4ac8b3', 0.125)\n",
      "('2e5a3f27c8799b2455fc99bd33fb396a', 0.125)\n",
      "('49587ef465a58c65439506103faa210c', 0.125)\n",
      "('85fca0881e606136eb7637b830850c84', 0.125)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1df5a0fbce075d47107feeb8d65a138b',\n",
       " '7db05f44445f307fd8157d7bc5e4db51',\n",
       " '47c701ff962c505d385bfd251f061c42',\n",
       " '68d0fcae54011b2f7d96f82ef4b0af2c',\n",
       " 'bd03a2df7386c06c1f7b3a3a8661ece4',\n",
       " '4740836db6ce4b2144d81fd6a9a796c9',\n",
       " '4bef82fe1a89bc190e1d2b0e2c4ac8b3',\n",
       " '2e5a3f27c8799b2455fc99bd33fb396a',\n",
       " '49587ef465a58c65439506103faa210c',\n",
       " '85fca0881e606136eb7637b830850c84']"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_mk3_manual('a18f526db904f8f2ee4c8d178bfc818c', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a18f526db904f8f2ee4c8d178bfc818c', {'7a040062b953a9a3e7fb1545def5773f'})"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_user_to_items.items())[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend_dummy(user, n_best = 10):\n",
    "    item_similarities = []\n",
    "    for item in all_items:\n",
    "        #пропустим те фильмы, которые пользователь уже просмотрел, если нас об этом попросили\n",
    "        if item in user_to_items[user]: continue\n",
    "        item_similarities.append(item)\n",
    "         \n",
    "    random.shuffle(item_similarities)\n",
    "    #вернём n_best наиболее пригодных\n",
    "    #print(items_sorted[:n_best])\n",
    "    return item_similarities[:n_best]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Оценка качества - map@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200\n",
      "100 / 200\n",
      "AP@10 = 9.259259259259259e-05\n"
     ]
    }
   ],
   "source": [
    "check_quality(recommend_dummy,10,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200\n",
      "100 / 200\n",
      "AP@10 = 0.0\n"
     ]
    }
   ],
   "source": [
    "check_quality(recommend_mk3_manual,10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200\n",
      "100 / 200\n",
      "AP@200 = 0.0020925098584587487\n"
     ]
    }
   ],
   "source": [
    "check_quality(generic_recommend(user_film_mk1),200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200\n",
      "100 / 200\n",
      "AP@100 = 0.007631459343212682\n"
     ]
    }
   ],
   "source": [
    "check_quality(generic_recommend(user_film_mk2),200,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200\n",
      "100 / 200\n",
      "AP@100 = 0.003063415596733874\n"
     ]
    }
   ],
   "source": [
    "check_quality(recommend,100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2732"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_user_to_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#сколько рекоммендаций рассматриваем\n",
    "def check_quality(function, K = 10, max_n_users = len(test_user_to_items)):\n",
    "    APatK_per_user = []\n",
    "    user_list = list(test_user_to_items.keys())[:max_n_users]\n",
    "    \n",
    "    for i, user in enumerate(user_list):\n",
    "        #фильмы, которые пользователю на самом деле нравятся\n",
    "        test_items = test_user_to_items[user]\n",
    "\n",
    "        #Выдать топ-K рекоммендаций\n",
    "        recommendation_list = function(user,n_best=K)\n",
    "        #Посчитать ap@k\n",
    "        user_APatK = apk(test_items, recommendation_list,k=K)\n",
    "\n",
    "        #и сложить в коробку\n",
    "        APatK_per_user.append(user_APatK)\n",
    "\n",
    "        #Progress bar\n",
    "        if i % 100 ==0:\n",
    "            print(i,'/',max_n_users)\n",
    "\n",
    "        if i > max_n_users:\n",
    "            break\n",
    "\n",
    "    print('AP@{} = {}'.format(K, sum(APatK_per_user)/len(APatK_per_user)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* Кроме качества рекоммендаций, map@k ещё зависит от доли тестовой выборки, фильтрации и от самого K. Сравнивать качество разных алгоритмов имеет смысл только при одинаковом K и тестовой выборке.\n",
    "* Давать полезные рекоммендации пользователям с малым числом просмотров тоже можно: например, можно выдавать наиболее популярные в целом фильмы.\n",
    "* Разделение на обучение/тест честнее делать на по времени: первые 70% (например) лайков в обучение, остальные в тест. Это ближе к реальной жизни, когда вы сначала обучаете модель на логах, а потом применяете на новых сессиях пользователей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import json\n",
    "from metrics import apk\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Словари для основной выборки\n",
    "user_to_items = defaultdict(set)\n",
    "item_to_users = defaultdict(set)\n",
    "edges = []\n",
    "#Словари для тестовой выборки\n",
    "test_user_to_items = defaultdict(set)\n",
    "test_item_to_users = defaultdict(set)\n",
    "test_edges = []\n",
    "\n",
    "with open(\"data/train_likes.csv\") as datafile:\n",
    "    for like in csv.DictReader(datafile):\n",
    "        # Кидаем монетку. В зависимости от результата кладём в обучение или тест\n",
    "        if random.random() < 0.5:\n",
    "            user_to_items[like['user_id']].add(like['item_id'])\n",
    "            item_to_users[like['item_id']].add(like['user_id'])\n",
    "            edges.append((like['user_id'], like['item_id']))\n",
    "        else:\n",
    "            test_user_to_items[like['user_id']].add(like['item_id'])\n",
    "            test_item_to_users[like['item_id']].add(like['user_id'])\n",
    "            test_edges.append((like['user_id'], like['item_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_items = set(item_to_users.keys()) | set(test_item_to_users.keys())\n",
    "all_users = set(user_to_items.keys()) | set(test_user_to_items.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_list = list(user_to_items.keys())\n",
    "test_users_list = list(test_user_to_items.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_to_i = {user: i for i, user in enumerate(all_users)}\n",
    "item_to_i = {item: i for i, item in enumerate(all_items)}\n",
    "all_users_list = list(all_users)\n",
    "all_items_list = list(all_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix = sp.sparse.lil_matrix((len(all_users), len(all_items)))\n",
    "test_matrix = sp.sparse.lil_matrix((len(all_users), len(all_items)))\n",
    "for user, items in user_to_items.items():\n",
    "    for item in items:\n",
    "        matrix[user_to_i[user], item_to_i[item]] = True\n",
    "for user, items in test_user_to_items.items():\n",
    "    for item in items:\n",
    "        test_matrix[user_to_i[user], item_to_i[item]] = True\n",
    "matrix = matrix.tocsr()\n",
    "test_matrix = test_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<55863x23891 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 55163 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u, s, vt = sp.sparse.linalg.svds(matrix.astype(np.float32), k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55863, 100), (100,), (100, 23891))"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape, s.shape, vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "films = json.load(open('data/items.json'))\n",
    "films = {a['id']:a for a in films}\n",
    "for a in films.values():\n",
    "    del a['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in films.values():\n",
    "    if 'genre' in f:\n",
    "        f[f['genre']] = 1\n",
    "        del f['genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = Counter()\n",
    "for f in films.values():\n",
    "    features.update(set(f.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('year', 138078),\n",
       " ('duration', 138078),\n",
       " (0, 34128),\n",
       " (3, 32154),\n",
       " ('f_114306', 30650),\n",
       " (1, 28635),\n",
       " ('f_79251', 16647),\n",
       " ('f_117573', 15295),\n",
       " ('f_138980', 15215),\n",
       " (5, 15008),\n",
       " ('f_64513', 14020),\n",
       " ('f_205162', 10693),\n",
       " ('f_122038', 10436),\n",
       " ('f_84602', 8377),\n",
       " ('f_68894', 8348),\n",
       " ('f_127793', 7946),\n",
       " ('f_72071', 7866),\n",
       " (4, 7386),\n",
       " ('f_210900', 7085),\n",
       " ('f_130202', 6371)]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_features_size = 60 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_features = set([_[0] for _ in features.most_common(small_features_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_to_i = {feature: i for i, feature in enumerate(small_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 6,\n",
       " 3: 3,\n",
       " 4: 4,\n",
       " 5: 5,\n",
       " 6: 8,\n",
       " 'f_84602': 2,\n",
       " 'f_127828': 10,\n",
       " 'f_79251': 9,\n",
       " 'f_83623': 12,\n",
       " 'f_93782': 14,\n",
       " 'f_151440': 15,\n",
       " 'f_165994': 18,\n",
       " 'f_46479': 17,\n",
       " 'f_69077': 38,\n",
       " 'f_64718': 28,\n",
       " 'f_49558': 19,\n",
       " 'f_93783': 20,\n",
       " 'f_429': 13,\n",
       " 'f_205164': 21,\n",
       " 'f_110758': 22,\n",
       " 'f_163241': 23,\n",
       " 'f_187493': 24,\n",
       " 'f_114306': 25,\n",
       " 7: 7,\n",
       " 'f_210900': 27,\n",
       " 'f_253': 29,\n",
       " 8: 11,\n",
       " 'f_71149': 30,\n",
       " 'f_64513': 26,\n",
       " 'f_140034': 31,\n",
       " 'f_72071': 32,\n",
       " 'f_44763': 33,\n",
       " 'f_173938': 40,\n",
       " 'f_121936': 36,\n",
       " 'f_109358': 37,\n",
       " 'duration': 58,\n",
       " 'f_127794': 39,\n",
       " 'year': 41,\n",
       " 'f_197169': 43,\n",
       " 'f_122038': 44,\n",
       " 'f_130202': 45,\n",
       " 'f_138980': 46,\n",
       " 'f_68894': 47,\n",
       " 'f_117573': 48,\n",
       " 'f_188358': 49,\n",
       " 'f_55703': 50,\n",
       " 'f_89959': 51,\n",
       " 'f_154169': 52,\n",
       " 'f_144994': 34,\n",
       " 'f_71282': 42,\n",
       " 'f_187216': 53,\n",
       " 'f_49716': 35,\n",
       " 'f_203955': 55,\n",
       " 'f_127793': 56,\n",
       " 'f_168177': 57,\n",
       " 'f_44580': 54,\n",
       " 'f_205162': 59,\n",
       " 'f_99688': 16}"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_to_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фильтрация пользователей\n",
    "* Значительная часть пользователей имеет всего 1-2 просмотра. При всём желании, рекоммендовать им что-либо осмысленное при помощи рассматриваемого здесь метода мы вряд ли сможем. Для простоты вычислений, удалим их из выборки.\n",
    "* Важно понимать, что качество на оставшихся пользователях скорее всего будет выше, чем на первоначальной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_items_per_user = 2\n",
    "from copy import copy\n",
    "for user in copy(test_user_to_items).keys():\n",
    "    \n",
    "    n_items_per_user = len(user_to_items[user]) + len(test_user_to_items[user])\n",
    "    \n",
    "    if n_items_per_user <= min_items_per_user:\n",
    "        del user_to_items[user]\n",
    "        del test_user_to_items[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рекоммендующая функция\n",
    "Позволим себе немного вольности: наша функция будет возвращать не вероятности, а список фильмов в порядке убывания \"рекомендованности\".\n",
    "\n",
    "* Рекоммендованность фильма item пользователю user посчитаем так:\n",
    "  * Для каждого фильма, полайканного пользователем user, найдём других людей, которым понравился фильм.\n",
    "  * Сложим всех таких \"друзей по лайкам\" вместе и назовём соседями (__neighborhood__) пользователя.\n",
    "  * Для фильма item узнаем его аудиторию - множество пользователей, которые его лайкнули\n",
    "  * Пригодность фильма пользователю - то, насколько \"друзьям по лайкам\" пользователя нравится этот фильм.\n",
    "\n",
    "Для примера, будем использовать косинусную меру расстояния\n",
    "  \n",
    "$ cos(u_{film}, u_{neighborhood}) = $ =$ u_{film} \\cdot u_{neighborhood} \\over |u_{film}| |u_{neighborhood}| $\n",
    "\n",
    "\n",
    "$u_{neighborhood}$ зависит только от пользователя, но не от фильма, поэтому при сравнении фильмов по пригодности для одного пользователя, его можно исключить из формулы для простоты вычислений.\n",
    "\n",
    "$ similarity(u_{film}, u_{neighborhood}) = $ $  u_{film} \\cdot u_{neighborhood} \\over |u_{film}| $\n",
    "  \n",
    "  \n",
    "Распишем формулу подробно:\n",
    "\n",
    "$ similarity(u_{film}, u_{neighborhood}) = $ $ \\sum _{u_i} [u_i \\in u_{film}] \\cdot [u_i \\in u_{neighborhood}] \\over |u_{film}|  $\n",
    "\n",
    "* u_i - очередной пользователь (в цикле по всем пользователям)\n",
    "  \n",
    "Выражение $[u_i \\in u_{neighborhood}]$ здесь означает \"сколько раз очередной пользователь входит в множество друзей по лайкам\"\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from collections import Counter\n",
    "\n",
    "def recommend(user, n_best = 10):\n",
    "    user_items = user_to_items[user]\n",
    "    \n",
    "    neighborhood = Counter()\n",
    "    for item in user_items:\n",
    "        neighborhood.update(item_to_users[item])\n",
    "    \n",
    "    #словарь {фильм -> пригодность фильма пользователю}\n",
    "    item_similarities = {}\n",
    "    \n",
    "    for item in all_items:\n",
    "        if item in user_items: continue\n",
    "        item_users = item_to_users[item]\n",
    "        if len(item_users) == 0: continue\n",
    "        \n",
    "        n_common_users = sum(neighborhood[user] for user in item_users)\n",
    "        similarity = float(n_common_users) / sqrt(len(item_users))\n",
    "        item_similarities[item] = similarity\n",
    "    \n",
    "    items_sorted = sorted(all_items, key = lambda x: item_similarities.get(x, 0),reverse = True)\n",
    "    \n",
    "    return items_sorted[:n_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_to_int = dict()\n",
    "for i, user in enumerate(all_users):\n",
    "    user_to_int[user] = i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Making dataset\n",
    "user_features = defaultdict(lambda:defaultdict(lambda:0))\n",
    "for user in list(user_to_items.keys())[0:]:\n",
    "    for item in user_to_items[user]:\n",
    "        if item in films:\n",
    "            for feature, value in films[item].items():    \n",
    "                user_features[user][feature]+=value\n",
    "    for f in user_features[user]:\n",
    "        user_features[user][f]/=len(user_to_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges[0] in test_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_size = 2*small_features_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(user=None, item=None, X_sample = np.zeros(features_size, dtype='float32')):\n",
    "    if user is not None:\n",
    "        for f in user_features[user].keys()&small_features:\n",
    "            X_sample[feature_to_i[f]] = user_features[user][f]\n",
    "        if item is not None:\n",
    "            X_sample[2*small_features_size+0] = user_film_mk2(user, item)\n",
    "    if item is not None and item in films:\n",
    "        for f in films[item].keys()&small_features:\n",
    "            X_sample[small_features_size+feature_to_i[f]] = films[item][f]\n",
    "    \n",
    "    return X_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55344"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_random_samples(X_size = 100, use_test=False):\n",
    "    X = np.zeros((X_size, features_size), dtype='float32')\n",
    "    Y = np.zeros(X_size, dtype='int8')\n",
    "    if use_test:\n",
    "        local_users_list = test_users_list\n",
    "        local_user_to_items = test_user_to_items\n",
    "    else:\n",
    "        local_users_list = users_list\n",
    "        local_user_to_items = user_to_items\n",
    "    for i in range(X_size):\n",
    "        film =''\n",
    "        while film not in films: \n",
    "            result = random.random()\n",
    "            if result > 0.50:\n",
    "                result = 1\n",
    "            else:\n",
    "                result = 0\n",
    "            user = random.choice(local_users_list)\n",
    "            if result==0:\n",
    "                film = random.choice(all_items_list)\n",
    "                if film in local_user_to_items[user]:\n",
    "                    result = 1\n",
    "            else:\n",
    "                film = random.choice(list(local_user_to_items[user]|{''}))\n",
    "        X[i] = extract_features(user, film)\n",
    "        Y[i] = result\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_samples(X_true = 100,  use_test=False):\n",
    "    X_fake=X_true\n",
    "    X = np.zeros((X_true+X_fake, features_size), dtype='float32')\n",
    "    Y = np.zeros(X_true+X_fake, dtype='float32')\n",
    "    if use_test:\n",
    "        local_users_list = test_users_list\n",
    "        local_user_to_items = test_user_to_items\n",
    "        local_edges = test_edges \n",
    "    else: \n",
    "        local_users_list = users_list\n",
    "        local_user_to_items = user_to_items\n",
    "        local_edges = edges\n",
    "    for i, (user, film)  in enumerate(local_edges[:X_true]):\n",
    "        X[i] = extract_features(user, film)\n",
    "        Y[i] = 1\n",
    "    for i in range(X_true, X_true+X_fake):\n",
    "        user = random.choice(local_users_list)\n",
    "        film = random.choice(all_items_list)\n",
    "        X[i] = extract_features(user, film)\n",
    "        if film in local_user_to_items[user]:\n",
    "            Y[i] = 1\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = generate_random_samples(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-254-947ff9859be2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/anaconda/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    288\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 290\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    798\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    656\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    348\u001b[0m                                            max_leaf_nodes)\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.878675890313\n",
      "Test score: 0.202766900421\n"
     ]
    }
   ],
   "source": [
    "X, Y = generate_random_samples(100, use_test=False)\n",
    "print(\"Train score: %s\" %(rf.score(X, Y),))\n",
    "X, Y = generate_random_samples(100, use_test=True)\n",
    "print(\"Test score: %s\" %(rf.score(X, Y),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_X = T.matrix(\"input X\")\n",
    "input_shape = [None, features_size]\n",
    "\n",
    "target_y = T.matrix(\"target Y integer\",dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Input layer (auxilary)\n",
    "layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "#fully connected layer, that takes input layer and applies 50 neurons to it.\n",
    "# nonlinearity here is sigmoid as in logistic regression\n",
    "# you can give a name to each layer (optional)\n",
    "layer = lasagne.layers.BatchNormLayer(layer)\n",
    "\n",
    "layer = lasagne.layers.DenseLayer(layer,num_units=50,\n",
    "                                   nonlinearity = lasagne.nonlinearities.sigmoid,\n",
    "                                   name = \"hidden_dense_layer\")\n",
    "layer = lasagne.layers.DenseLayer(layer,num_units=50,\n",
    "                                   nonlinearity = lasagne.nonlinearities.sigmoid,\n",
    "                                   name = \"hidden_dense_layer\")\n",
    "layer = lasagne.layers.DenseLayer(layer,num_units=20,\n",
    "                                   nonlinearity = lasagne.nonlinearities.sigmoid,\n",
    "                                   name = \"hidden_dense_layer\")\n",
    "\n",
    "layer = lasagne.layers.DropoutLayer(layer, p=0.5)\n",
    "\n",
    "dense_output = lasagne.layers.DenseLayer(layer,num_units = 1 ,\n",
    "                                        nonlinearity = lasagne.nonlinearities.tanh,\n",
    "                                        name='output', b=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[beta, gamma, hidden_dense_layer.W, hidden_dense_layer.b, hidden_dense_layer.W, hidden_dense_layer.b, hidden_dense_layer.W, hidden_dense_layer.b, output.W]\n"
     ]
    }
   ],
   "source": [
    "#network prediction (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(dense_output)\n",
    "#all network weights (shared variables)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output, trainable=True)\n",
    "\n",
    "predict = theano.function([input_X], y_predicted)\n",
    "print (all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = lasagne.objectives.squared_error(y_predicted, target_y)\n",
    "loss = lasagne.objectives.aggregate(loss, mode = 'mean')\n",
    "\n",
    "\n",
    "updates_sgd = lasagne.updates.adadelta(loss, all_weights,learning_rate=0.01)\n",
    "\n",
    "train_fun = theano.function([input_X,target_y],loss,updates= updates_sgd)\n",
    "\n",
    "\n",
    "loss_fun = theano.function([input_X,target_y],loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 47.874s\n",
      "  train loss: 0.248482666684\n",
      "  test loss: 0.256554608581\n",
      "Epoch 2 of 100 took 46.722s\n",
      "  train loss: 0.247284901165\n",
      "  test loss: 0.256372477171\n",
      "Epoch 3 of 100 took 46.761s\n",
      "  train loss: 0.245874528896\n",
      "  test loss: 0.255463886883\n",
      "Epoch 4 of 100 took 50.006s\n",
      "  train loss: 0.243981953537\n",
      "  test loss: 0.255082078194\n",
      "Epoch 5 of 100 took 49.138s\n",
      "  train loss: 0.24262902697\n",
      "  test loss: 0.254432885154\n",
      "Epoch 6 of 100 took 48.138s\n",
      "  train loss: 0.240568305502\n",
      "  test loss: 0.254004824513\n",
      "Epoch 7 of 100 took 47.468s\n",
      "  train loss: 0.238661476063\n",
      "  test loss: 0.252689206136\n",
      "Epoch 8 of 100 took 49.617s\n",
      "  train loss: 0.236162299211\n",
      "  test loss: 0.251836812979\n",
      "Epoch 9 of 100 took 47.268s\n",
      "  train loss: 0.233667052396\n",
      "  test loss: 0.250873459204\n",
      "Epoch 10 of 100 took 48.685s\n",
      "  train loss: 0.23065448692\n",
      "  test loss: 0.249171837162\n",
      "Epoch 11 of 100 took 47.167s\n",
      "  train loss: 0.226591415263\n",
      "  test loss: 0.248092779274\n",
      "Epoch 12 of 100 took 51.655s\n",
      "  train loss: 0.223122254222\n",
      "  test loss: 0.246947911421\n",
      "Epoch 13 of 100 took 44.367s\n",
      "  train loss: 0.218757055197\n",
      "  test loss: 0.245348148509\n",
      "Epoch 14 of 100 took 45.219s\n",
      "  train loss: 0.214378925795\n",
      "  test loss: 0.242865707744\n",
      "Epoch 15 of 100 took 48.812s\n",
      "  train loss: 0.208348360872\n",
      "  test loss: 0.241125508222\n",
      "Epoch 16 of 100 took 42.109s\n",
      "  train loss: 0.203410196432\n",
      "  test loss: 0.238841976119\n",
      "Epoch 17 of 100 took 49.840s\n",
      "  train loss: 0.196619323661\n",
      "  test loss: 0.236286346926\n",
      "Epoch 18 of 100 took 46.333s\n",
      "  train loss: 0.19031847788\n",
      "  test loss: 0.234831132987\n",
      "Epoch 19 of 100 took 44.031s\n",
      "  train loss: 0.184760069672\n",
      "  test loss: 0.23127491921\n",
      "Epoch 20 of 100 took 44.922s\n",
      "  train loss: 0.178505622808\n",
      "  test loss: 0.2281316848\n",
      "Epoch 21 of 100 took 43.668s\n",
      "  train loss: 0.172534277438\n",
      "  test loss: 0.226535666286\n",
      "Epoch 22 of 100 took 45.196s\n",
      "  train loss: 0.166381735072\n",
      "  test loss: 0.223067682797\n",
      "Epoch 23 of 100 took 46.758s\n",
      "  train loss: 0.162478834181\n",
      "  test loss: 0.220472153524\n",
      "Epoch 24 of 100 took 42.939s\n",
      "  train loss: 0.157562217765\n",
      "  test loss: 0.216860048955\n",
      "Epoch 25 of 100 took 43.473s\n",
      "  train loss: 0.153430249514\n",
      "  test loss: 0.214995355933\n",
      "Epoch 26 of 100 took 49.020s\n",
      "  train loss: 0.149389919031\n",
      "  test loss: 0.211260077505\n",
      "Epoch 27 of 100 took 43.909s\n",
      "  train loss: 0.14471644152\n",
      "  test loss: 0.210019629396\n",
      "Epoch 28 of 100 took 44.428s\n",
      "  train loss: 0.14116910656\n",
      "  test loss: 0.208774670248\n",
      "Epoch 29 of 100 took 45.778s\n",
      "  train loss: 0.137513247614\n",
      "  test loss: 0.204726971293\n",
      "Epoch 30 of 100 took 46.962s\n",
      "  train loss: 0.135579286509\n",
      "  test loss: 0.204685545013\n",
      "Epoch 31 of 100 took 44.626s\n",
      "  train loss: 0.131589415521\n",
      "  test loss: 0.201794116593\n",
      "Epoch 32 of 100 took 47.544s\n",
      "  train loss: 0.131953289471\n",
      "  test loss: 0.199411350323\n",
      "Epoch 33 of 100 took 43.348s\n",
      "  train loss: 0.127711962393\n",
      "  test loss: 0.19929982116\n",
      "Epoch 34 of 100 took 45.443s\n",
      "  train loss: 0.126206780016\n",
      "  test loss: 0.1977246551\n",
      "Epoch 35 of 100 took 43.992s\n",
      "  train loss: 0.12362873076\n",
      "  test loss: 0.195501571467\n",
      "Epoch 36 of 100 took 46.341s\n",
      "  train loss: 0.123552258256\n",
      "  test loss: 0.194521028246\n",
      "Epoch 37 of 100 took 46.462s\n",
      "  train loss: 0.119402871887\n",
      "  test loss: 0.194132058025\n",
      "Epoch 38 of 100 took 44.275s\n",
      "  train loss: 0.117884864819\n",
      "  test loss: 0.190731486915\n",
      "Epoch 39 of 100 took 46.301s\n",
      "  train loss: 0.115540079978\n",
      "  test loss: 0.191885963735\n",
      "Epoch 40 of 100 took 46.261s\n",
      "  train loss: 0.113751654497\n",
      "  test loss: 0.190081219436\n",
      "Epoch 41 of 100 took 45.345s\n",
      "  train loss: 0.114950477182\n",
      "  test loss: 0.189164457035\n",
      "Epoch 42 of 100 took 48.143s\n",
      "  train loss: 0.112589884368\n",
      "  test loss: 0.189769860549\n",
      "Epoch 43 of 100 took 51.737s\n",
      "  train loss: 0.110149159837\n",
      "  test loss: 0.186652418836\n",
      "Epoch 44 of 100 took 52.003s\n",
      "  train loss: 0.109771971685\n",
      "  test loss: 0.185947939287\n",
      "Epoch 45 of 100 took 45.815s\n",
      "  train loss: 0.109944197948\n",
      "  test loss: 0.187949724922\n",
      "Epoch 46 of 100 took 49.134s\n",
      "  train loss: 0.10831991195\n",
      "  test loss: 0.186507095367\n",
      "Epoch 47 of 100 took 44.633s\n",
      "  train loss: 0.10536213853\n",
      "  test loss: 0.184156869263\n",
      "Epoch 48 of 100 took 47.085s\n",
      "  train loss: 0.105808108002\n",
      "  test loss: 0.183072224266\n",
      "Epoch 49 of 100 took 46.538s\n",
      "  train loss: 0.104735329196\n",
      "  test loss: 0.185450775108\n",
      "Epoch 50 of 100 took 47.007s\n",
      "  train loss: 0.105188154556\n",
      "  test loss: 0.183186383014\n",
      "Epoch 51 of 100 took 43.266s\n",
      "  train loss: 0.102694611801\n",
      "  test loss: 0.181389874883\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 100 #amount of passes through the data\n",
    "\n",
    "batch_size = 1000 #number of samples processed at each function call\n",
    "num_batches = 300\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in (generate_samples(batch_size, use_test=False) for _ in range(num_batches)):\n",
    "        inputs, targets = batch\n",
    "        targets = targets.reshape((-1, 1))\n",
    "        train_err_batch = train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_batches = 0\n",
    "    for batch in (generate_samples(batch_size, use_test=True) for _ in range(num_batches)):\n",
    "        inputs, targets = batch\n",
    "        targets = targets.reshape((-1, 1))\n",
    "        val_err += loss_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  train loss: %s\" % (train_err / train_batches, ))\n",
    "    print(\"  test loss: %s\" % (val_err / train_batches, ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.99576976],\n",
       "        [ 0.34379049],\n",
       "        [ 0.45306789],\n",
       "        [ 0.18065817],\n",
       "        [ 0.87963364],\n",
       "        [ 0.32685928],\n",
       "        [ 0.30049887],\n",
       "        [ 0.66587964],\n",
       "        [ 0.94503155],\n",
       "        [ 0.66958117]]), array([1, 0, 0, 0, 1, 0, 0, 0, 0, 0], dtype=int8))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = generate_random_samples(10, use_test=True)\n",
    "predict(X), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 200\n",
    "batch_size = 1000\n",
    "total_batch = 200\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, features_size]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 1]) \n",
    "\n",
    "W = tf.Variable(tf.zeros([features_size, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "pred = tf.nn.tanh(tf.matmul(x, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(tf.squared_difference(y, pred))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.197359944\n",
      "Epoch: 0002 cost= 0.196920267\n",
      "Epoch: 0003 cost= 0.196819510\n",
      "Epoch: 0004 cost= 0.196147946\n",
      "Epoch: 0005 cost= 0.196093600\n",
      "Epoch: 0006 cost= 0.195149677\n",
      "Epoch: 0007 cost= 0.194767732\n",
      "Epoch: 0008 cost= 0.194793865\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-0ed0d057c661>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Loop over all batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_random_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Fit training using batch data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-c73a02c546d3>\u001b[0m in \u001b[0;36mgenerate_random_samples\u001b[1;34m(X_size, use_test)\u001b[0m\n\u001b[0;32m     22\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0mfilm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_user_to_items\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m|\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = generate_random_samples(batch_size)\n",
    "        batch_ys = batch_ys.reshape((-1, 1))\n",
    "        # Fit training using batch data\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                      y: batch_ys})\n",
    "        # Compute average loss\n",
    "        avg_cost += c / total_batch\n",
    "    # Display logs per epoch step\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "print (\"Optimization Finished!\")\n",
    "\n",
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy for 3000 examples\n",
    "x_test, y_test =   generate_random_samples(batch_size, use_test=True)\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "print (\"Accuracy:\", cost.eval({x: x_test, y: y_test}, session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.39263499e-06],\n",
       "       [  1.58444379e-06],\n",
       "       [  1.72093326e-06],\n",
       "       [  1.30214175e-06],\n",
       "       [  5.04144646e-06],\n",
       "       [  2.78425546e-06],\n",
       "       [  1.67713119e-06],\n",
       "       [  3.35766458e-06],\n",
       "       [  1.04063997e-06],\n",
       "       [  3.70196062e-06],\n",
       "       [  1.53262081e-05],\n",
       "       [  1.56174860e-06],\n",
       "       [  1.44628314e-06],\n",
       "       [ -2.96540602e-05],\n",
       "       [  1.19626009e-06],\n",
       "       [  1.27234807e-06],\n",
       "       [  3.34417655e-06],\n",
       "       [  9.86953410e-07],\n",
       "       [  2.64779883e-05],\n",
       "       [  5.46258798e-06],\n",
       "       [  4.35487069e-02],\n",
       "       [  4.35487069e-02],\n",
       "       [  2.58548814e-03],\n",
       "       [  4.35487069e-02],\n",
       "       [ -8.12742580e-03],\n",
       "       [  4.35487069e-02],\n",
       "       [  3.76427569e-03],\n",
       "       [  2.90592201e-03],\n",
       "       [  4.35487069e-02],\n",
       "       [  1.27365021e-03],\n",
       "       [  4.00641002e-03],\n",
       "       [  1.73721742e-02],\n",
       "       [  1.39680263e-02],\n",
       "       [  1.39039569e-02],\n",
       "       [  1.31673804e-02],\n",
       "       [  1.03020733e-02],\n",
       "       [  2.82581579e-02],\n",
       "       [ -1.68155297e-03],\n",
       "       [  4.12299559e-02],\n",
       "       [  4.08005714e-03],\n",
       "       [  9.21867013e-01]], dtype=float32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.10983541],\n",
       "        [ 0.10352055],\n",
       "        [ 0.20060187],\n",
       "        [ 0.18288017],\n",
       "        [ 0.04566551],\n",
       "        [ 0.19504457],\n",
       "        [ 0.16255407],\n",
       "        [ 0.19504296],\n",
       "        [ 0.00110639],\n",
       "        [ 0.37141779]]), array([0, 0, 0, 1, 1, 0, 1, 0, 1, 0], dtype=int8))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 0.40177947],\n",
       "         [ 0.35771608],\n",
       "         [ 0.35028359],\n",
       "         [ 0.38270113],\n",
       "         [ 0.34994081],\n",
       "         [ 0.39418095],\n",
       "         [ 0.34223077],\n",
       "         [ 0.38585007],\n",
       "         [ 0.42586172],\n",
       "         [ 0.39199695]], dtype=float32)],\n",
       " array([0, 0, 0, 0, 1, 1, 0, 0, 1, 1], dtype=int8))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = generate_random_samples(10, use_test=False)\n",
    "sess.run([pred], feed_dict={x: X}), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.98906996e-05,   0.00000000e+00,   5.98360493e-05,\n",
       "          0.00000000e+00,   0.00000000e+00,   3.98906996e-05,\n",
       "          0.00000000e+00,   7.97813991e-05,   1.99453498e-05,\n",
       "          9.97267489e-05,   3.98906996e-05,   3.98906996e-05,\n",
       "         -3.40250190e-05,   1.99453498e-05,   1.99453498e-05,\n",
       "          8.33385002e-06,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.99453498e-05,   1.99453498e-05,   1.00000000e+00,\n",
       "          1.00000000e+00,   2.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          2.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   2.00000000e+00,  -6.53573573e-01,\n",
       "          2.00000000e+00,   1.00000000e+00,   4.65432197e-01,\n",
       "          1.00000000e+00,   1.00000000e+00,   2.00000000e+00,\n",
       "          1.00000000e+00,   1.09823048e-03,   0.00000000e+00], dtype=float32),\n",
       " array([ 0.74074074,  0.09090909,  0.10869565,  0.06      ,  0.12727273,\n",
       "         0.12962963,  0.11111111,  0.12121212,  0.26470588,  0.14925373]),\n",
       " array([1, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=int8))"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = generate_random_samples(10, use_test=True)\n",
    "X[0], rf.predict(X), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_film_mk3(user, item, debug=False):\n",
    "    X = np.zeros((1, features_size), dtype='float32')\n",
    "    X[0] = extract_features(user, item)\n",
    "    probs = rf.predict(X)\n",
    "    if debug:\n",
    "        print(probs)\n",
    "    return probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_film_mk1(user, item, debug=False):\n",
    "    #print(len(user_features[user]))\n",
    "    cursum = 0\n",
    "    curcnt = 0\n",
    "    if item in films:\n",
    "        for feature, value in films[item].items():\n",
    "            if type(value) is int:\n",
    "                curcnt+=1    \n",
    "                cursum+=value*user_features[user][feature]\n",
    "    if curcnt==0:\n",
    "        curcnt+=1\n",
    "    #cursum/=curcnt\n",
    "    if debug:\n",
    "        print(cursum)\n",
    "    return cursum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_film_mk2(user, item, debug=False):\n",
    "    ui = user_to_i[user]\n",
    "    ii = item_to_i[item]\n",
    "    if debug:\n",
    "        print(ui, ii)\n",
    "    a = np.dot(u[ui,:] * s, vt[:,ii])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_film_mk3('26052b20aa96ed8d803dbfe4e9497192', '45ea2aa2143effcb6575daf0143e31b4', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recommend_mk3_manual(user, n_best = 10, debug=False):\n",
    "    user_items = user_to_items[user]\n",
    "\n",
    "    item_similarities = {}\n",
    "    cur_items = [item for item in all_items if item not in user_items and len(item_to_users[item])>0]\n",
    "    \n",
    "    X = np.zeros((len(cur_items), features_size), dtype='float32')\n",
    "    X_user = extract_features(user)\n",
    "    for i, item in enumerate(cur_items):\n",
    "        X[i] = extract_features(user=None, item=item, X_sample=X_user)\n",
    "    \n",
    "    probs =rf.predict(X)\n",
    "    item_similarities = {key: prob for key, prob in zip(cur_items, probs)}\n",
    "    items_sorted = sorted(all_items, key = lambda x: item_similarities.get(x, 0),reverse = True)\n",
    "    if debug:\n",
    "        print(X)\n",
    "        for a in  items_sorted[:n_best]:\n",
    "            print((a, item_similarities.get(a, 0)))\n",
    "    return items_sorted[:n_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend_mk4_manual(user, n_best = 10, debug=False):\n",
    "    user_items = user_to_items[user]\n",
    "\n",
    "    item_similarities = {}\n",
    "    cur_items = [item for item in all_items if item not in user_items and len(item_to_users[item])>0]\n",
    "    \n",
    "    X = np.zeros((len(cur_items), features_size), dtype='float32')\n",
    "    X_user = extract_features(user)\n",
    "    for i, item in enumerate(cur_items):\n",
    "        X[i] = extract_features(user=None, item=item, X_sample=X_user)\n",
    "    \n",
    "    probs =predict(X)\n",
    "    item_similarities = {key: prob for key, prob in zip(cur_items, probs)}\n",
    "    items_sorted = sorted(all_items, key = lambda x: item_similarities.get(x, 0),reverse = True)\n",
    "    if debug:\n",
    "        print(X)\n",
    "        for a in  items_sorted[:n_best]:\n",
    "            print((a, item_similarities.get(a, 0)))\n",
    "    return items_sorted[:n_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generic_recommend(user_flim_function):\n",
    "    def recommend(user, n_best = 10, debug=False):\n",
    "        user_items = user_to_items[user]\n",
    "\n",
    "        item_similarities = {}\n",
    "        for item in all_items:\n",
    "            if item in user_items: continue\n",
    "            item_users = item_to_users[item]\n",
    "            if len(item_users) == 0: continue\n",
    "\n",
    "            item_similarities[item] = user_flim_function(user, item)\n",
    "\n",
    "        items_sorted = sorted(all_items, key = lambda x: item_similarities.get(x, 0),reverse = True)\n",
    "        if debug:\n",
    "            for a in  items_sorted[:n_best]:\n",
    "                print((a, item_similarities.get(a, 0)))\n",
    "        return items_sorted[:n_best]\n",
    "    return recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('7682d6641caf183ffcf2616c9bb1e434', 0.08739379)\n",
      "('b5e424608d041f38c2fd6125b3bcd40f', 0.062321573)\n",
      "('006a602774c79d476cdbd5dff7dd24f1', 0.047887065)\n",
      "('e9caaa12a277abf87ba07b84e09a4170', 0.045560129)\n",
      "('aa5f2ca699da42e467e550f9f071fb3f', 0.044249389)\n",
      "('bcee3a68dbfb6b4ea97ac53ceb1d3287', 0.044186898)\n",
      "('e8047b4262e676d28f50816ac3fde1ca', 0.042720947)\n",
      "('77c7998b3d3d41f3fc3aec786b4015ac', 0.038098648)\n",
      "('1da6f84598fbeea050da49fe37297932', 0.037306551)\n",
      "('2dc12b93072f94bca9438902bddc36c4', 0.035170436)\n",
      "10976 651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0024645075"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_recommend(user_film_mk2)('1a337111f63f6a7f86cebf6b2ad3d732', debug=True)\n",
    "user_film_mk2('1a337111f63f6a7f86cebf6b2ad3d732', 'c745ed11b94ed93f3008897c75240de3', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e13df82b3252eaeefc0c13599695f9c7', 0.000658196541476355)\n",
      "('634e622e5c957c2cd8ddb98ca75ef937', 0.0005584697927678163)\n",
      "('8faa6327a2ca2eb92e738ffc204d35c0', 0.0005584697927678163)\n",
      "('64b6536649e20bd72d7183ec717b6c86', 0.0005385244430261086)\n",
      "('5c6d1862dc1e82f340962b308999e603', 0.0005185790932844008)\n",
      "('bcee3a68dbfb6b4ea97ac53ceb1d3287', 0.0005185790932844007)\n",
      "('ef786a839c609561b76ce03768eace80', 0.0005185790932844007)\n",
      "('ff987a60d68f5bbbefe97f8c5062711a', 0.0004986337435426931)\n",
      "('753bce7ac492e6c95b07b85f8877cf5a', 0.0004786883938009853)\n",
      "('7b0f1b0815292c0a550fabd5eeadc011', 0.00045874304405927767)\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_recommend(user_film_mk1)('1a337111f63f6a7f86cebf6b2ad3d732', debug=True)\n",
    "user_film_mk1('1a337111f63f6a7f86cebf6b2ad3d732', 'c745ed11b94ed93f3008897c75240de3', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.42418862e-05   2.05451302e-04   6.84837723e-05 ...,   3.32924634e-01\n",
      "    1.00000000e+00   1.80774500e-16]\n",
      " [  3.42418862e-05   2.05451302e-04   6.84837723e-05 ...,   3.32924634e-01\n",
      "    1.00000000e+00   1.80774500e-16]\n",
      " [  3.42418862e-05   2.05451302e-04   6.84837723e-05 ...,   3.32924634e-01\n",
      "    1.00000000e+00   1.80774500e-16]\n",
      " ..., \n",
      " [  3.42418862e-05   2.05451302e-04   6.84837723e-05 ...,   4.15132821e-01\n",
      "    1.00000000e+00   1.80774500e-16]\n",
      " [  3.42418862e-05   2.05451302e-04   6.84837723e-05 ...,   4.15132821e-01\n",
      "    1.00000000e+00   1.80774500e-16]\n",
      " [  3.42418862e-05   2.05451302e-04   6.84837723e-05 ...,   2.63475370e+00\n",
      "    1.00000000e+00   1.80774500e-16]]\n",
      "('ab3279ea6dd62acd5a9c330a396c3b1a', array([ 1.]))\n",
      "('634e622e5c957c2cd8ddb98ca75ef937', array([ 1.]))\n",
      "('2c3754f00e6d27cb51d240ae4addd120', array([ 1.]))\n",
      "('c3adb5ff8f46fcc5b2ab864c4de5023b', array([ 1.]))\n",
      "('e9eefe65f665c414c361c226c0ca4641', array([ 1.]))\n",
      "('6223bf138ae3bc3eeb93c8331e834df3', array([ 1.]))\n",
      "('106756aba5d378cf731f6683545a586c', array([ 1.]))\n",
      "('fa7a26762b9645be36a0913e73a7cd82', array([ 0.99999999]))\n",
      "('15464106e8fe48bb01b9daef7696f52f', array([ 0.99999999]))\n",
      "('9de1905753afad63dfcb7c33720e9c24', array([ 0.99999997]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ab3279ea6dd62acd5a9c330a396c3b1a',\n",
       " '634e622e5c957c2cd8ddb98ca75ef937',\n",
       " '2c3754f00e6d27cb51d240ae4addd120',\n",
       " 'c3adb5ff8f46fcc5b2ab864c4de5023b',\n",
       " 'e9eefe65f665c414c361c226c0ca4641',\n",
       " '6223bf138ae3bc3eeb93c8331e834df3',\n",
       " '106756aba5d378cf731f6683545a586c',\n",
       " 'fa7a26762b9645be36a0913e73a7cd82',\n",
       " '15464106e8fe48bb01b9daef7696f52f',\n",
       " '9de1905753afad63dfcb7c33720e9c24']"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_mk4_manual('a18f526db904f8f2ee4c8d178bfc818c', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a18f526db904f8f2ee4c8d178bfc818c', {'7a040062b953a9a3e7fb1545def5773f'})"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_user_to_items.items())[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend_dummy(user, n_best = 10):\n",
    "    item_similarities = []\n",
    "    for item in all_items:\n",
    "        #пропустим те фильмы, которые пользователь уже просмотрел, если нас об этом попросили\n",
    "        if item in user_to_items[user]: continue\n",
    "        item_similarities.append(item)\n",
    "         \n",
    "    random.shuffle(item_similarities)\n",
    "    #вернём n_best наиболее пригодных\n",
    "    #print(items_sorted[:n_best])\n",
    "    return item_similarities[:n_best]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Оценка качества - map@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200\n",
      "100 / 200\n",
      "AP@10 = 9.259259259259259e-05\n"
     ]
    }
   ],
   "source": [
    "check_quality(recommend_dummy,10,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200\n",
      "100 / 200\n",
      "AP@10 = 0.0\n"
     ]
    }
   ],
   "source": [
    "check_quality(recommend_mk3_manual,10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200\n",
      "100 / 200\n",
      "AP@100 = 1.1261261261261261e-05\n"
     ]
    }
   ],
   "source": [
    "check_quality(recommend_mk4_manual,100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200\n",
      "100 / 200\n",
      "AP@100 = 0.0014817629179331307\n"
     ]
    }
   ],
   "source": [
    "check_quality(generic_recommend(user_film_mk1),100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200\n",
      "100 / 200\n",
      "AP@100 = 0.007631459343212682\n"
     ]
    }
   ],
   "source": [
    "check_quality(generic_recommend(user_film_mk2),200,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200\n",
      "100 / 200\n",
      "AP@100 = 0.003063415596733874\n"
     ]
    }
   ],
   "source": [
    "check_quality(recommend,100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2732"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_user_to_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#сколько рекоммендаций рассматриваем\n",
    "def check_quality(function, K = 10, max_n_users = len(test_user_to_items)):\n",
    "    APatK_per_user = []\n",
    "    user_list = list(test_user_to_items.keys())[:max_n_users]\n",
    "    \n",
    "    for i, user in enumerate(user_list):\n",
    "        #фильмы, которые пользователю на самом деле нравятся\n",
    "        test_items = test_user_to_items[user]\n",
    "\n",
    "        #Выдать топ-K рекоммендаций\n",
    "        recommendation_list = function(user,n_best=K)\n",
    "        #Посчитать ap@k\n",
    "        user_APatK = apk(test_items, recommendation_list,k=K)\n",
    "\n",
    "        #и сложить в коробку\n",
    "        APatK_per_user.append(user_APatK)\n",
    "\n",
    "        #Progress bar\n",
    "        if i % 100 ==0:\n",
    "            print(i,'/',max_n_users)\n",
    "\n",
    "        if i > max_n_users:\n",
    "            break\n",
    "\n",
    "    print('AP@{} = {}'.format(K, sum(APatK_per_user)/len(APatK_per_user)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* Кроме качества рекоммендаций, map@k ещё зависит от доли тестовой выборки, фильтрации и от самого K. Сравнивать качество разных алгоритмов имеет смысл только при одинаковом K и тестовой выборке.\n",
    "* Давать полезные рекоммендации пользователям с малым числом просмотров тоже можно: например, можно выдавать наиболее популярные в целом фильмы.\n",
    "* Разделение на обучение/тест честнее делать на по времени: первые 70% (например) лайков в обучение, остальные в тест. Это ближе к реальной жизни, когда вы сначала обучаете модель на логах, а потом применяете на новых сессиях пользователей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
